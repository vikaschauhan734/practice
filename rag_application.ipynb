{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b50148f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faf92f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"yolov11.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b45c4f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'yolov11.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content='YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL\\nENHANCEMENTS\\nRahima Khanam* and Muhammad Hussain\\nDepartment of Computer Science, Huddersfield University, Queensgate, Huddersfield HD1 3DH, UK;\\n*Correspondence: rahima.khanam@hud.ac.uk;\\nOctober 24, 2024\\nABSTRACT\\nThis study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only\\nLook Once) series of object detection models. We examine the models architectural innovations,\\nincluding the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial\\nPyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) com-\\nponents, which contribute in improving the models performance in several ways such as enhanced\\nfeature extraction. The paper explores YOLOv11’s expanded capabilities across various computer\\nvision tasks, including object detection, instance segmentation, pose estimation, and oriented object\\ndetection (OBB). We review the model’s performance improvements in terms of mean Average\\nPrecision (mAP) and computational efficiency compared to its predecessors, with a focus on the\\ntrade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11’s\\nversatility across different model sizes, from nano to extra-large, catering to diverse application needs\\nfrom edge devices to high-performance computing environments. Our research provides insights into\\nYOLOv11’s position within the broader landscape of object detection and its potential impact on\\nreal-time computer vision applications.\\nKeywords Automation; Computer Vision; YOLO; YOLOV11; Object Detection; Real-Time Image processing; YOLO\\nversion comparison\\n1 Introduction\\nComputer vision, a rapidly advancing field, enables machines to interpret and understand visual data [ 1]. A crucial\\naspect of this domain is object detection[2], which involves the precise identification and localization of objects within\\nimages or video streams[3]. Recent years have witnessed remarkable progress in algorithmic approaches to address this\\nchallenge [4].\\nA pivotal breakthrough in object detection came with the introduction of the You Only Look Once (YOLO) algorithm\\nby Redmon et al. in 2015 [5]. This innovative approach, as its name suggests, processes the entire image in a single pass\\nto detect objects and their locations. YOLO’s methodology diverges from traditional two-stage detection processes by\\nframing object detection as a regression problem [5]. It employs a single convolutional neural network to simultaneously\\npredict bounding boxes and class probabilities across the entire image [6], streamlining the detection pipeline compared\\nto more complex traditional methods.\\nYOLOv11 is the latest iteration in the YOLO series, building upon the foundation established by YOLOv1. Unveiled at\\nthe YOLO Vision 2024 (YV24) conference, YOLOv11 represents a significant leap forward in real-time object detection\\ntechnology. This new version introduces substantial enhancements in both architecture and training methodologies,\\npushing the boundaries of accuracy, speed, and efficiency.\\nYOLOv11’s innovative design incorporates advanced feature extraction techniques, allowing for more nuanced detail\\ncapture while maintaining a lean parameter count. This results in improved accuracy across a diverse range of computer\\narXiv:2410.17725v1  [cs.CV]  23 Oct 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'yolov11.pdf', 'total_pages': 9, 'page': 1, 'page_label': '2'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nvision (CV) tasks, from object detection to classification. Furthermore, YOLOv11 achieves remarkable gains in\\nprocessing speed, substantially enhancing real-time performance capabilities.\\nIn the following sections, this paper will provide a comprehensive analysis of YOLOv11’s architecture, exploring its\\nkey components and innovations. We will examine the evolution of YOLO models, leading up to the development\\nof YOLOv11. The study will delve into the model’s expanded capabilities across various CV tasks, including object\\ndetection, instance segmentation, pose estimation, and oriented object detection. We will also review YOLOv11’s\\nperformance improvements in terms of accuracy and computational efficiency compared to its predecessors, with a\\nparticular focus on its versatility across different model sizes. Finally, we will discuss the potential impact of YOLOv11\\non real-time CV applications and its position within the broader landscape of object detection technologies.\\n2 Evolution of YOLO models\\nTable 1 illustrates the progression of YOLO models from their inception to the most recent versions. Each iteration has\\nbrought significant improvements in object detection capabilities, computational efficiency, and versatility in handling\\nvarious CV tasks.\\nTable 1: YOLO: Evolution of models\\nRelease Year Tasks Contributions Framework\\nYOLO [5] 2015 Object Detection, Basic Classifica-\\ntion\\nSingle-stage object detector Darknet\\nYOLOv2 [7] 2016 Object Detection, Improved Classi-\\nfication\\nMulti-scale training, dimension clus-\\ntering\\nDarknet\\nYOLOv3 [8] 2018 Object Detection, Multi-scale Detec-\\ntion\\nSPP block, Darknet-53 backbone Darknet\\nYOLOv4 [9] 2020 Object Detection, Basic Object\\nTracking\\nMish activation, CSPDarknet-53\\nbackbone\\nDarknet\\nYOLOv5 [10] 2020 Object Detection, Basic Instance\\nSegmentation (via custom modifica-\\ntions)\\nAnchor-free detection, SWISH acti-\\nvation, PANet\\nPyTorch\\nYOLOv6 [11] 2022 Object Detection, Instance Segmen-\\ntation\\nSelf-attention, anchor-free OD PyTorch\\nYOLOv7 [12] 2022 Object Detection, Object Tracking,\\nInstance Segmentation\\nTransformers, E-ELAN reparame-\\nterisation\\nPyTorch\\nYOLOv8 [13] 2023 Object Detection, Instance Segmen-\\ntation, Panoptic Segmentation, Key-\\npoint Estimation\\nGANs, anchor-free detection PyTorch\\nYOLOv9 [14] 2024 Object Detection, Instance Segmen-\\ntation\\nPGI and GELAN PyTorch\\nYOLOv10 [15] 2024 Object Detection Consistent dual assignments for\\nNMS-free training\\nPyTorch\\nThis evolution showcases the rapid advancement in object detection technologies, with each version introducing novel\\nfeatures and expanding the range of supported tasks. From the original YOLO’s groundbreaking single-stage detection\\nto YOLOv10’s NMS-free training, the series has consistently pushed the boundaries of real-time object detection.\\nThe latest iteration, YOLO11, builds upon this legacy with further enhancements in feature extraction, efficiency,\\nand multi-task capabilities. Our subsequent analysis will delve into YOLO11’s architectural innovations, including\\nits improved backbone and neck structures, and its performance across various computer vision tasks such as object\\ndetection, instance segmentation, and pose estimation.\\n3 What is YOLOv11?\\nThe evolution of the YOLO algorithm reaches new heights with the introduction of YOLOv11 [ 16], representing a\\nsignificant advancement in real-time object detection technology. This latest iteration builds upon the strengths of its\\npredecessors while introducing novel capabilities that expand its utility across diverse CV applications.\\nYOLOv11 distinguishes itself through its enhanced adaptability, supporting an expanded range of CV tasks beyond\\ntraditional object detection. Notable among these are posture estimation and instance segmentation, broadening the\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'yolov11.pdf', 'total_pages': 9, 'page': 2, 'page_label': '3'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nmodel’s applicability in various domains. YOLOv11’s design focuses on balancing power and practicality, aiming to\\naddress specific challenges across various industries with increased accuracy and efficiency.\\nThis latest model demonstrates the ongoing evolution of real-time object detection technology, pushing the boundaries\\nof what’s possible in CV applications. Its versatility and performance improvements position YOLOv11 as a significant\\nadvancement in the field, potentially opening new avenues for real-world implementation across diverse sectors.\\n4 Architectural footprint of Yolov11\\nThe YOLO framework revolutionized object detection by introducing a unified neural network architecture that\\nsimultaneously handles both bounding box regression and object classification tasks [17]. This integrated approach\\nmarked a significant departure from traditional two-stage detection methods, offering end-to-end training capabilities\\nthrough its fully differentiable design.\\nAt its core, the YOLO architecture consists of three fundamental components. First, the backbone serves as the primary\\nfeature extractor, utilizing convolutional neural networks to transform raw image data into multi-scale feature maps.\\nSecond, the neck component acts as an intermediate processing stage, employing specialized layers to aggregate\\nand enhance feature representations across different scales. Third, the head component functions as the prediction\\nmechanism, generating the final outputs for object localization and classification based on the refined feature maps.\\nBuilding on this established architecture, YOLO11 extends and enhances the foundation laid by YOLOv8, introducing\\narchitectural innovations and parameter optimizations to achieve superior detection performance as illustrated in Figure\\n1. The following sections detail the key architectural modifications implemented in YOLO11:\\nFigure 1: Key architectural modules in YOLO11\\n4.1 Backbone\\nThe backbone is a crucial component of the YOLO architecture, responsible for extracting features from the input\\nimage at multiple scales. This process involves stacking convolutional layers and specialized blocks to generate feature\\nmaps at various resolutions.\\n4.1.1 Convolutional Layers\\nYOLOv11 maintains a structure similar to its predecessors, utilizing initial convolutional layers to downsample the\\nimage. These layers form the foundation of the feature extraction process, gradually reducing spatial dimensions while\\nincreasing the number of channels. A significant improvement in YOLO11 is the introduction of the C3k2 block,\\nwhich replaces the C2f block used in previous versions [ 18]. The C3k2 block is a more computationally efficient\\nimplementation of the Cross Stage Partial (CSP) Bottleneck. It employs two smaller convolutions instead of one large\\nconvolution, as seen in YOLOv8 [13]. The \"k2\" in C3k2 indicates a smaller kernel size, which contributes to faster\\nprocessing while maintaining performance.\\n4.1.2 SPPF and C2PSA\\nYOLO11 retains the Spatial Pyramid Pooling - Fast (SPPF) block from previous versions but introduces a new Cross\\nStage Partial with Spatial Attention (C2PSA) block after it [18]. The C2PSA block is a notable addition that enhances\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'yolov11.pdf', 'total_pages': 9, 'page': 3, 'page_label': '4'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nspatial attention in the feature maps. This spatial attention mechanism allows the model to focus more effectively on\\nimportant regions within the image. By pooling features spatially, the C2PSA block enables YOLO11 to concentrate on\\nspecific areas of interest, potentially improving detection accuracy for objects of varying sizes and positions.\\n4.2 Neck\\nThe neck combines features at different scales and transmits them to the head for prediction. This process typically\\ninvolves upsampling and concatenation of feature maps from different levels, enabling the model to capture multi-scale\\ninformation effectively.\\n4.2.1 C3k2 Block\\nYOLO11 introduces a significant change by replacing the C2f block in the neck with the C3k2 block. The C3k2 block\\nis designed to be faster and more efficient, enhancing the overall performance of the feature aggregation process. After\\nupsampling and concatenation, the neck in YOLO11 incorporates this improved block, resulting in enhanced speed and\\nperformance [18].\\n4.2.2 Attention Mechanism\\nA notable addition to YOLO11 is its increased focus on spatial attention through the C2PSA module. This attention\\nmechanism enables the model to concentrate on key regions within the image, potentially leading to more accurate\\ndetection, especially for smaller or partially occluded objects. The inclusion of C2PSA sets YOLO11 apart from its\\npredecessor, YOLOv8, which lacks this specific attention mechanism [18].\\n4.3 Head\\nThe head of YOLOv11 is responsible for generating the final predictions in terms of object detection and classification.\\nIt processes the feature maps passed from the neck, ultimately outputting bounding boxes and class labels for objects\\nwithin the image.\\n4.3.1 C3k2 Block\\nIn the head section, YOLOv11 utilizes multiple C3k2 blocks to efficiently process and refine the feature maps. The\\nC3k2 blocks are placed in several pathways within the head, functioning to process multi-scale features at different\\ndepths. The C3k2 block exhibits flexibility depending on the value of the c3k parameter:\\n• When c3k = False, the C3k2 module behaves similarly to the C2f block, utilizing a standard bottleneck\\nstructure.\\n• When c3k = True, the bottleneck structure is replaced by the C3 module, which allows for deeper and more\\ncomplex feature extraction.\\nKey characteristics of the C3k2 block:\\n• Faster processing: The use of two smaller convolutions reduces the computational overhead compared to a\\nsingle large convolution, leading to quicker feature extraction.\\n• Parameter efficiency: C3k2 is a more compact version of the CSP bottleneck, making the architecture more\\nefficient in terms of the number of trainable parameters.\\nAnother notable addition is the C3k block, which offers enhanced flexibility by allowing customizable kernel sizes. The\\nadaptability of C3k is particularly useful for extracting more detailed features from images, contributing to improved\\ndetection accuracy.\\n4.3.2 CBS Blocks\\nThe head of YOLOv11 includes several CBS (Convolution-BatchNorm-Silu) [19] layers after the C3k2 blocks. These\\nlayers further refine the feature maps by:\\n• Extracting relevant features for accurate object detection.\\n• Stabilizing and normalizing the data flow through batch normalization.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'yolov11.pdf', 'total_pages': 9, 'page': 4, 'page_label': '5'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n• Utilizing the Sigmoid Linear Unit (SiLU) activation function for non-linearity, which improves model perfor-\\nmance.\\nCBS blocks serve as foundational components in both feature extraction and the detection process, ensuring that the\\nrefined feature maps are passed to the subsequent layers for bounding box and classification predictions.\\n4.3.3 Final Convolutional Layers and Detect Layer\\nEach detection branch ends with a set of Conv2D layers, which reduce the features to the required number of outputs for\\nbounding box coordinates and class predictions. The final Detect layer consolidates these predictions, which include:\\n• Bounding box coordinates for localizing objects in the image.\\n• Objectness scores that indicate the presence of objects.\\n• Class scores for determining the class of the detected object.\\n5 Key Computer Vision Tasks Supported by YOLO11\\nYOLO11 supports a diverse range of CV tasks, showcasing its versatility and power in various applications. Here’s an\\noverview of the key tasks:\\n1. Object Detection: YOLO11 excels in identifying and localizing objects within images or video frames,\\nproviding bounding boxes for each detected item [ 20]. This capability finds applications in surveillance\\nsystems, autonomous vehicles, and retail analytics, where precise object identification is crucial [21].\\n2. Instance Segmentation: Going beyond simple detection, YOLO11 can identify and separate individual\\nobjects within an image down to the pixel level [20]. This fine-grained segmentation is particularly valuable in\\nmedical imaging for precise organ or tumor delineation, and in manufacturing for detailed defect detection\\n[21].\\n3. Image Classification: YOLOv11 is capable of classifying entire images into predetermined categories,\\nmaking it ideal for applications like product categorization in e-commerce platforms or wildlife monitoring in\\necological studies [21].\\n4. Pose Estimation: The model can detect specific key points within images or video frames to track movements\\nor poses. This capability is beneficial for fitness tracking applications, sports performance analysis, and various\\nhealthcare applications requiring motion assessment [21].\\n5. Oriented Object Detection (OBB): YOLO11 introduces the ability to detect objects with an orientation angle,\\nallowing for more precise localization of rotated objects. This feature is especially valuable in aerial imagery\\nanalysis, robotics, and warehouse automation tasks where object orientation is crucial [21].\\n6. Object Tracking: It identifies and traces the path of objects in a sequence of images or video frames[ 21].\\nThis real-time tracking capability is essential for applications such as traffic monitoring, sports analysis, and\\nsecurity systems.\\nTable 2 outlines the YOLOv11 model variants and their corresponding tasks. Each variant is designed for specific\\nuse cases, from object detection to pose estimation. Moreover, all variants support core functionalities like inference,\\nvalidation, training, and export, making YOLOv11 a versatile tool for various CV applications.\\n6 Advancements and Key Features of YOLOv11\\nYOLOv11 represents a significant advancement in object detection technology, building upon the foundations laid by\\nits predecessors, YOLOv9 and YOLOv10, which were introduced earlier in 2024. This latest iteration from Ultralytics\\nshowcases enhanced architectural designs, more sophisticated feature extraction techniques, and refined training\\nmethodologies. The synergy of YOLOv11’s rapid processing, high accuracy, and computational efficiency positions it\\nas one of the most formidable models in Ultralytics’ portfolio to date [22]. A key strength of YOLOv11 lies in its refined\\narchitecture, which facilitates the detection of subtle details even in challenging scenarios. The model’s improved\\nfeature extraction capabilities allow it to identify and process a broader range of patterns and intricate elements within\\nimages. Compared to earlier versions, YOLOv11 introduces several notable enhancements:\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'yolov11.pdf', 'total_pages': 9, 'page': 5, 'page_label': '6'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nTable 2: YOLOv11 Model Variants and Tasks\\nModel Variants Task Inference Validation Training Export\\nYOLOv11 yolo11-nano yolo11-small\\nyolo11-medium yolo11-\\nlarge yolo11-xlarge\\nDetection ✓ ✓ ✓ ✓\\nYOLOv11-seg yolo11-nano-seg yolo11-\\nsmall-seg yolo11-medium-\\nseg yolo11-large-seg\\nyolo11-xlarge-seg\\nInstance Segmen-\\ntation\\n✓ ✓ ✓ ✓\\nYOLOv11-pose yolo11-nano-pose yolo11-\\nsmall-pose yolo11-medium-\\npose yolo11-large-pose\\nyolo11-xlarge-pose\\nPose/Keypoints ✓ ✓ ✓ ✓\\nYOLOv11-obb yolo11-nano-obb yolo11-\\nsmall-obb yolo1-medium-\\nobb yolo11-large-obb\\nyolo11-xlarge-obb\\nOriented Detec-\\ntion\\n✓ ✓ ✓ ✓\\nYOLOv11-cls yolo11-nano-cls yolo11-\\nsmall-cls yolo11-medium-\\ncls yolo11-large-cls yolo11-\\nxlarge-cls\\nClassification ✓ ✓ ✓ ✓\\n1. Enhanced precision with reduced complexity: The YOLOv11m variant achieves superior mean Average\\nPrecision (mAP) scores on the COCO dataset while utilizing 22% fewer parameters than its YOLOv8m\\ncounterpart, demonstrating improved computational efficiency without compromising accuracy [23].\\n2. Versatility in CV tasks: YOLOv11 exhibits proficiency across a diverse array of CV applications, including\\npose estimation, object recognition, image classification, instance segmentation, and oriented bounding box\\n(OBB) detection [23].\\n3. Optimized speed and performance: Through refined architectural designs and streamlined training pipelines,\\nYOLOv11 achieves faster processing speeds while maintaining a balance between accuracy and computational\\nefficiency [23].\\n4. Streamlined parameter count: The reduction in parameters contributes to faster model performance without\\nsignificantly impacting the overall accuracy of YOLOv11 [22].\\n5. Advanced feature extraction: YOLOv11 incorporates improvements in both its backbone and neck architec-\\ntures, resulting in enhanced feature extraction capabilities and, consequently, more precise object detection\\n[23].\\n6. Contextual adaptability: YOLOv11 demonstrates versatility across various deployment scenarios, including\\ncloud platforms, edge devices, and systems optimized for NVIDIA GPUs [23].\\nYOLOv11 model demonstrates significant advancements in both inference speed and accuracy compared to its\\npredecessors. In the benchmark analysis, YOLOv11 was compared against several of its predecessors including variants\\nsuch as YOLOv5 [24] through to the more recent variants such as YOLOv10. As presented in Figure 2, YOLOv11\\nconsistently outperforms these models, achieving superior mAP on the COCO dataset while maintaining a faster\\ninference rate [25].\\nThe performance comparison graph depicted in Figure 2 overs several key insights. The YOLOv11 variants (11n, 11s,\\n11m, and 11x) form a distinct performance frontier, with each model achieving higher COCO mAP 50−95 scores at\\ntheir respective latency points. Notably, the YOLOv11x achieves approximately 54.5% mAP50−95 at 13ms latency,\\nsurpassing all previous YOLO iterations. The intermediate variants, particularly YOLOv11m, demonstrate exceptional\\nefficiency by achieving comparable accuracy to larger models from previous generations while requiring significantly\\nless processing time.\\nA particularly noteworthy observation is the performance leap in the low-latency regime (2-6ms), where YOLOv11s\\nmaintains high accuracy (approximately 47% mAP50−95) while operating at speeds previously associated with much\\nless accurate models. This represents a crucial advancement for real-time applications where both speed and accuracy\\nare critical. The improvement curve of YOLOv11 also shows better scaling characteristics across its model variants,\\nsuggesting more efficient utilization of additional computational resources compared to previous generations.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'yolov11.pdf', 'total_pages': 9, 'page': 6, 'page_label': '7'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\nFigure 2: Benchmarking YOLOv11 Against Previous Versions [23]\\n7 Discussion\\nYOLO11 marks a significant leap forward in object detection technology, building upon its predecessors while\\nintroducing innovative enhancements. This latest iteration demonstrates remarkable versatility and efficiency across\\nvarious CV tasks.\\n1. Efficiency and Scalability: YOLO11 introduces a range of model sizes, from nano to extra-large, catering\\nto diverse application needs. This scalability allows for deployment in scenarios ranging from resource-\\nconstrained edge devices to high-performance computing environments. The nano variant, in particular,\\nshowcases impressive speed and efficiency improvements over its predecessor, making it ideal for real-time\\napplications.\\n2. Architectural Innovations: The model incorporates novel architectural elements that enhance its feature\\nextraction and processing capabilities. The incorporation of novel elements such as the C3k2 block, SPPF, and\\nC2PSA contributes to more effective feature extraction and processing. These enhancements allow the model\\nto better analyze and interpret complex visual information, potentially leading to improved detection accuracy\\nacross various scenarios.\\n3. Multi-Task Proficiency: YOLO11’s versatility extends beyond object detection, encompassing tasks such as\\ninstance segmentation, image classification, pose estimation, and oriented object detection. This multi-faceted\\napproach positions YOLO11 as a comprehensive solution for diverse CV challenges.\\n4. Enhanced Attention Mechanisms: A key advancement in YOLO11 is the integration of sophisticated spatial\\nattention mechanisms, particularly the C2PSA component. This feature enables the model to focus more\\neffectively on critical regions within an image, enhancing its ability to detect and analyze objects. The\\nimproved attention capability is especially beneficial for identifying complex or partially occluded objects,\\naddressing a common challenge in object detection tasks. This refinement in spatial awareness contributes to\\nYOLO11’s overall performance improvements, particularly in challenging visual environments.\\n5. Performance Benchmarks: Comparative analyses reveal YOLO11’s superior performance, particularly in its\\nsmaller variants. The nano model, despite a slight increase in parameters, demonstrates enhanced inference\\nspeed and frames per second (FPS) compared to its predecessor. This improvement suggests that YOLO11\\nachieves a favorable balance between computational efficiency and detection accuracy.\\n6. Implications for Real-World Applications: The advancements in YOLO11 have significant implications\\nfor various industries. Its improved efficiency and multi-task capabilities make it particularly suitable for\\napplications in autonomous vehicles, surveillance systems, and industrial automation. The model’s ability to\\nperform well across different scales also opens up new possibilities for deployment in resource-constrained\\nenvironments without compromising on performance.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'yolov11.pdf', 'total_pages': 9, 'page': 7, 'page_label': '8'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n8 Conclusion\\nYOLOv11 represents a significant advancement in the field of CV , offering a compelling combination of enhanced\\nperformance and versatility. This latest iteration of the YOLO architecture demonstrates marked improvements in\\naccuracy and processing speed, while simultaneously reducing the number of parameters required. Such optimizations\\nmake YOLOv11 particularly well-suited for a wide range of applications, from edge computing to cloud-based analysis.\\nThe model’s adaptability across various tasks, including object detection, instance segmentation, and pose estimation,\\npositions it as a valuable tool for diverse industries such as emotion detection [26], healthcare [27] and various other\\nindustries [17]. Its seamless integration capabilities and improved efficiency make it an attractive option for businesses\\nseeking to implement or upgrade their CV systems. In summary, YOLOv11’s blend of enhanced feature extraction,\\noptimized performance, and broad task support establishes it as a formidable solution for addressing complex visual\\nrecognition challenges in both research and practical applications.\\nReferences\\n[1] Milan Sonka, Vaclav Hlavac, and Roger Boyle. Image processing, analysis and machine vision. Springer, 2013.\\n[2] Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye. Object detection in 20 years: A survey.\\nProceedings of the IEEE, 111(3):257–276, 2023.\\n[3] Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Object detection with deep learning: A review.\\nIEEE transactions on neural networks and learning systems, 30(11):3212–3232, 2019.\\n[4] Muhammad Hussain and Rahima Khanam. In-depth review of yolov1 to yolov10 variants for enhanced photo-\\nvoltaic defect detection. In Solar, volume 4, pages 351–386. MDPI, 2024.\\n[5] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object\\ndetection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779–788,\\n2016.\\n[6] Juan Du. Understanding of object detection based on cnn family and yolo. In Journal of Physics: Conference\\nSeries, volume 1004, page 012029. IOP Publishing, 2018.\\n[7] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pages 7263–7271, 2017.\\n[8] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.\\n[9] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. Yolov4: Optimal speed and accuracy of\\nobject detection. arXiv preprint arXiv:2004.10934, 2020.\\n[10] Roboflow Blog Jacob Solawetz. What is yolov5? a guide for beginners., 2020. Accessed: 21 October 2024.\\n[11] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng,\\nWeiqiang Nie, et al. Yolov6: A single-stage object detection framework for industrial applications. arXiv preprint\\narXiv:2209.02976, 2022.\\n[12] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets new\\nstate-of-the-art for real-time object detectors. In Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, pages 7464–7475, 2023.\\n[13] Francesco Jacob Solawetz. What is yolov8? the ultimate guide, 2023. Accessed: 21 October 2024.\\n[14] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. Yolov9: Learning what you want to learn using\\nprogrammable gradient information. arXiv preprint arXiv:2402.13616, 2024.\\n[15] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yolov10: Real-time\\nend-to-end object detection. arXiv preprint arXiv:2405.14458, 2024.\\n[16] Glenn Jocher and Jing Qiu. Ultralytics yolo11, 2024.\\n[17] Rahima Khanam, Muhammad Hussain, Richard Hill, and Paul Allen. A comprehensive review of convolutional\\nneural networks for defect detection in industrial applications. IEEE Access, 2024.\\n[18] Satya Mallick. Yolo - learnopencv. https://learnopencv.com/yolo11/, 2024. Accessed: 2024-10-21.\\n[19] Jingwen Feng, Qiaofeng An, Jiahao Zhang, Shuxun Zhou, Guangwei Du, and Kai Yang. Application of yolov7-tiny\\nin the detection of steel surface defects. In 2024 5th International Seminar on Artificial Intelligence, Networking\\nand Information Technology (AINIT), pages 2241–2245. IEEE, 2024.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-24T00:37:53+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-24T00:37:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'yolov11.pdf', 'total_pages': 9, 'page': 8, 'page_label': '9'}, page_content='R.K HANAM ET AL .: YOLO V11: A N OVERVIEW OF THE KEY ARCHITECTURAL ENHANCEMENTS - OCTOBER 24,\\n2024\\n[20] Ultralytics. Instance segmentation and tracking, 2024. Accessed: 2024-10-21.\\n[21] Ultralytics Abirami Vina. Ultralytics yolo11 has arrived: Redefine what’s possible in ai, 2024. Accessed:\\n2024-10-21.\\n[22] Viso.AI Gaudenz Boesch. Yolov11: A new iteration of “you only look once. https://viso.ai/\\ncomputer-vision/yolov11/, 2024. Accessed: 2024-10-21.\\n[23] Ultralytics. Ultralytics yolov11. https://docs.ultralytics.com/models/yolo11/s, 2024. Accessed:\\n21-Oct-2024.\\n[24] Rahima Khanam and Muhammad Hussain. What is yolov5: A deep look into the internal features of the popular\\nobject detector. arXiv preprint arXiv:2407.20892, 2024.\\n[25] DigitalOcean. What’s new in yolov11 transforming object detection once again part 1, 2024. Accessed: 2024-10-\\n21.\\n[26] Muhammad Hussain and Hussain Al-Aqrabi. Child emotion recognition via custom lightweight cnn architecture.\\nIn Kids Cybersecurity Using Computational Intelligence Techniques, pages 165–174. Springer, 2023.\\n[27] Burcu Ataer Aydin, Muhammad Hussain, Richard Hill, and Hussain Al-Aqrabi. Domain modelling for a\\nlightweight convolutional network focused on automated exudate detection in retinal fundus images. In 2023 9th\\nInternational Conference on Information Technology Trends (ITT), pages 145–150. IEEE, 2023.\\n9')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "887d4760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65e8ca04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  40\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print(\"Total number of documents: \", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e39836",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_chroma'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_chroma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_google_genai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GoogleGenerativeAIEmbeddings\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_chroma'"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c8d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d61c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorspace = Chroma.from_documents(documents=docs, embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baad975",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorspace.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "retrieved_docs = retriever.invoke(\"What is new in yolov11?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eac871d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model_name=\"Llama3-8b-8192\", temperature=0, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1c64340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e7dfaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74d3f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689929e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is new in YOLOv11?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
